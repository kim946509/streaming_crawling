"""
í¬ë¡¤ë§ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬
"""
import logging
from datetime import date
from typing import List, Dict, Any

from crawling_view.common.song_service import SongService
from crawling_view.platform_crawlers import create_crawler
from streaming_site_list.models import SongInfo

logger = logging.getLogger(__name__)


class CrawlingManager:
    """
    í¬ë¡¤ë§ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, target_date: date = None):
        """
        í¬ë¡¤ë§ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            target_date: í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ (Noneì´ë©´ ì˜¤ëŠ˜)
        """
        self.target_date = target_date or date.today()
        self.active_songs: List[SongInfo] = []
        
    def run_full_crawling(self, platforms: List[str] = None, save_csv: bool = True, save_db: bool = True) -> Dict[str, Any]:
        """
        ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        
        Args:
            platforms: í¬ë¡¤ë§í•  í”Œë«í¼ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  í”Œë«í¼)
            save_csv: CSV ì €ì¥ ì—¬ë¶€
            save_db: DB ì €ì¥ ì—¬ë¶€
            
        Returns:
            Dict: ê° í”Œë«í¼ë³„ í¬ë¡¤ë§ ê²°ê³¼
        """
        logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {self.target_date}")
        
        # 1ë‹¨ê³„: í™œì„±í™”ëœ ë…¸ë˜ë“¤ ì¡°íšŒ
        self._load_active_songs()
        
        if not self.active_songs:
            logger.warning("âŒ í¬ë¡¤ë§ ëŒ€ìƒ ê³¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        # 2ë‹¨ê³„: í”Œë«í¼ë³„ í¬ë¡¤ë§ ì‹¤í–‰
        results = self._execute_platform_crawling(platforms, save_csv, save_db)
        
        # 3ë‹¨ê³„: ê²°ê³¼ ìš”ì•½
        self._log_summary(results)
        
        return results
    
    def _load_active_songs(self):
        """1ë‹¨ê³„: í™œì„±í™”ëœ ë…¸ë˜ë“¤ ì¡°íšŒ"""
        logger.info("ğŸ“‹ 1ë‹¨ê³„: í™œì„±í™”ëœ ë…¸ë˜ë“¤ ì¡°íšŒ ì¤‘...")
        
        self.active_songs = SongService.get_active_songs(self.target_date)
        
        logger.info(f"âœ… ì¡°íšŒ ì™„ë£Œ: {len(self.active_songs)}ê°œ ê³¡")
        for song in self.active_songs:
            logger.debug(f"   - {song.id}: {song.genie_artist} - {song.genie_title}")
    
    def _execute_platform_crawling(self, platforms: List[str], save_csv: bool, save_db: bool) -> Dict[str, Any]:
        """2ë‹¨ê³„: í”Œë«í¼ë³„ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("ğŸ”„ 2ë‹¨ê³„: í”Œë«í¼ë³„ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘...")
        
        # ê¸°ë³¸ í”Œë«í¼ ì„¤ì •
        if platforms is None:
            platforms = ['genie', 'youtube_music', 'youtube']
        
        results = {}
        
        for platform in platforms:
            if platform not in ['genie', 'youtube_music', 'youtube']:
                logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” í”Œë«í¼: {platform}")
                continue
            
            try:
                # í”Œë«í¼ë³„ í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
                crawler = create_crawler(platform, self.active_songs)
                platform_result = crawler.crawl(save_csv, save_db)
                results[platform] = platform_result
                
            except Exception as e:
                logger.error(f"âŒ {platform} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                results[platform] = {'error': str(e)}
        
        return results
    
    def _log_summary(self, results: Dict[str, Any]):
        """3ë‹¨ê³„: ê²°ê³¼ ìš”ì•½ ë¡œê·¸"""
        logger.info("ğŸ“Š 3ë‹¨ê³„: í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 60)
        logger.info(f"ğŸ¯ ëŒ€ìƒ ê³¡ ìˆ˜: {len(self.active_songs)}")
        
        for platform, result in results.items():
            if 'error' in result:
                logger.error(f"âŒ {platform}: ì˜¤ë¥˜ ë°œìƒ - {result['error']}")
            else:
                crawling_result = result.get('crawling_result', [])
                elapsed_time = result.get('elapsed_time', 0)
                
                if isinstance(crawling_result, list):
                    success_count = len(crawling_result)
                elif isinstance(crawling_result, dict):
                    success_count = len(crawling_result)
                else:
                    success_count = 0
                
                logger.info(f"âœ… {platform}: {success_count}ê°œ ì„±ê³µ ({elapsed_time:.2f}ì´ˆ)")
        
        logger.info("=" * 60)
        logger.info("ğŸ í¬ë¡¤ë§ ì™„ë£Œ!")


def run_crawling(target_date: date = None, platforms: List[str] = None, save_csv: bool = True, save_db: bool = True) -> Dict[str, Any]:
    """
    í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜ (í¸ì˜ í•¨ìˆ˜)
    
    Args:
        target_date: í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ
        platforms: í¬ë¡¤ë§í•  í”Œë«í¼ ë¦¬ìŠ¤íŠ¸
        save_csv: CSV ì €ì¥ ì—¬ë¶€
        save_db: DB ì €ì¥ ì—¬ë¶€
        
    Returns:
        Dict: í¬ë¡¤ë§ ê²°ê³¼
    """
    manager = CrawlingManager(target_date)
    return manager.run_full_crawling(platforms, save_csv, save_db) 